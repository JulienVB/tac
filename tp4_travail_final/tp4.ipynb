{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4: Travail Final \n",
    "## VANBELLE Julien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import webtext\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('webtext')\n",
    "## sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "## spacy\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n",
    "## textblob\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "## gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "## misc\n",
    "from IPython.display import Image\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import os\n",
    "import collections\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## dir\n",
    "data_path = \"/Users/julienvanbelle/Documents/GitHub/tac/data\"\n",
    "txt_path = data_path + '/txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(txt_path)\n",
    "data_bxl = [f for f in files if f.startswith('Bxl_')]\n",
    "len(data_bxl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for txt in data_bxl:\n",
    "    with open(os.path.join(txt_path, txt), 'r', encoding = \"ISO-8859-1\") as f:\n",
    "         data_list.append(f.read())\n",
    "\n",
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire tout le contenu dans un fichier temporaire pour le nettoyer des stopwords propre au corpus\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "with open(os.path.join(data_path, f'_temp.txt'), 'w') as f:\n",
    "    f.write(' '.join(data_list))\n",
    "    print(\"temp file saved in\",data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_words = webtext.words('{}/_temp.txt'.format(data_path))\n",
    "data_analysis = nltk.FreqDist(wt_words)\n",
    " \n",
    "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n",
    "data_analysis = nltk.FreqDist(filter_words)\n",
    "data_analysis.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addsw = []\n",
    "for z in sorted(filter_words):\n",
    "  if filter_words[z] > 25000:\n",
    "   addsw.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "swLower = ' '.join(str(e).lower() for e in addsw)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += stopwords.words(\"dutch\")\n",
    "sw += addsw\n",
    "sw += swLower\n",
    "sw += \"conseil communal\", \"conseil général\", \"conseil supérieur\", \"administration communale\", \"conseil provincial\", \"l'administration communale\", \"conseil\", \"echevin\" , \"messieurs\", \"bruxelles\", \"bourgmestre\", \"collège\", \"être\", \"prã\", \"collã\", \"van\", \"ãªtre\", \"crã\", \"dit\",\"intã\" , \"annã\", \"sociã\",\"mãªme\", \"aprã\", \"penses\", \"pense\", \"considã\", \"trã\", \"cembre\",\"elles\",\"alors\",\"rant\",\"res\",\"faite\",\"cole\",\"tat\",\"ance\",\"ment\",\"non\",\"vrier\",\"sident\",\"pital\",\"tait\",\"cet\",\"etc\",\"ves\"\n",
    "sw = set(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(u, folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\".txt\"\n",
    "        output_path = f\"_clean.txt\"\n",
    "    else:\n",
    "        ##for u in data_bxl:\n",
    "            input_path = f\"{folder}\" + \"/\" + u\n",
    "            output_path = data_path + \"/txt_cleaned/\" + \"Clean_\" + u ##opti ici avec {folder}\n",
    "            ##print(\"Cleaning ==> \" +u)\n",
    "    \n",
    "            output = open(output_path, \"w\", encoding=\"ISO-8859-1\")\n",
    "            with open(input_path, encoding=\"ISO-8859-1\") as g:\n",
    "                text = g.read()\n",
    "                words = nltk.wordpunct_tokenize(text)\n",
    "                kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw and \"ã\" not in w.lower()]\n",
    "                kept_string = \" \".join(kept)\n",
    "                output.write(kept_string)\n",
    "            return f'Output has been written in {data_path + \"/txt_cleaned\"}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(data_path +\"/txt_cleaned\"):\n",
    "    os.mkdir(data_path + \"/txt_cleaned\")\n",
    "for x in data_bxl:\n",
    "    clean_text(x, folder=txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le résultat\n",
    "with open(os.path.join(data_path + \"/txt_cleaned/\" + \"Clean_\" + data_bxl[0]), 'r', encoding=\"ISO-8859-1\") as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path = data_path + \"/txt_cleaned/\"\n",
    "\n",
    "clean_files = os.listdir(clean_path)\n",
    "clean_list = []\n",
    "for j in clean_files:\n",
    "    with open(os.path.join(clean_path, j), 'r', encoding = \"ISO-8859-1\") as f:\n",
    "        clean_list.append(f.read())\n",
    "\n",
    "len(clean_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Répartition du corpus en clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tfidf_vectors = vectorizer.fit_transform(clean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 3\n",
    "\n",
    "km_model = KMeans(n_clusters=N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km_model.fit_predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(clusters):\n",
    "    clustering[label].append(clean_list[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordClouds des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count1 = word_tokenize(str(list(clustering[0])))\n",
    "frequencies1 = Counter(cluster_count1)\n",
    "\n",
    "cluster_count2 = word_tokenize(str(list(clustering[1])))\n",
    "frequencies2 = Counter(cluster_count2)\n",
    "\n",
    "cluster_count3 = word_tokenize(str(list(clustering[2])))\n",
    "frequencies3 = Counter(cluster_count3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster n°1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=1000, height=500, background_color='white').generate_from_frequencies(frequencies1)\n",
    "cloud.to_file(os.path.join(data_path, \"cluster1.png\"))\n",
    "print(\"WordCloud du cluster n°1\")\n",
    "Image(filename=os.path.join(data_path, \"cluster1.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster n°2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=1000, height=500, background_color='white').generate_from_frequencies(frequencies2)\n",
    "cloud.to_file(os.path.join(data_path, \"cluster2.png\"))\n",
    "print(\"WordCloud du cluster n°2\")\n",
    "Image(filename=os.path.join(data_path, \"cluster2.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster n°3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cloud = WordCloud(width=1000, height=500, background_color='white').generate_from_frequencies(frequencies3)\n",
    "cloud.to_file(os.path.join(data_path, \"cluster3.png\"))\n",
    "print(\"WordCloud du cluster n°3\")\n",
    "Image(filename=os.path.join(data_path, \"cluster3.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de la thématique des tramways bruxellois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apparition du terme \"tramways\" dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tramdoc = []\n",
    "def check_if_exists(x,y, ls, r):\n",
    "    if x and y in ls:\n",
    "        tramdoc.append(r)\n",
    "\n",
    "cleanNum = os.listdir(clean_path)\n",
    "\n",
    "for q in cleanNum:\n",
    "    with open(os.path.join(clean_path, q), 'r', encoding = \"ISO-8859-1\") as j:\n",
    "         jlist = [(j.read())]\n",
    "         word_tokenize(str(jlist))\n",
    "         jstring = ''.join(str(jlist))\n",
    "         \n",
    "         check_if_exists('tramways','tramway', jstring, q)\n",
    " \n",
    "      \n",
    "print(\"Searching process done !\")\n",
    "print(\"The word is present in \"+ str(len(tramdoc)) + \"/\" + str(len(clean_list)) + \" documents\")\n",
    "print(\"The word appears in \"+ str(tramdoc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un directory contenant les documents qui comportent le mot \"tramways\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_path +\"/tramwaysRelated\"):\n",
    "    os.mkdir(data_path + \"/tramwaysRelated\")\n",
    "\n",
    "for t in tramdoc:\n",
    "  tramOut = open(data_path + \"/tramwaysRelated/\" + t, \"w\", encoding=\"ISO-8859-1\")\n",
    "  with open(clean_path+\"/\"+t, encoding=\"ISO-8859-1\") as l:\n",
    "      textTram = l.read()\n",
    "      tramOut.write(textTram)\n",
    "\n",
    "tram_path = data_path + \"/tramwaysRelated\"\n",
    "print(str(len(os.listdir(tram_path)))+ \" documents are saved in the /tramwaysRelated folder !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un fichier \"all\" contenant l'ensemble du corpus réduit \"tramways\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire tout le contenu dans un fichier temporaire\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "\n",
    "output = open(os.path.join(data_path, \"tramAll.txt\"), 'w')\n",
    "for d in tramdoc:\n",
    " with open(os.path.join(clean_path, d), 'r', encoding = \"ISO-8859-1\") as k:\n",
    "     klist = [(k.read())]\n",
    "     output.write(''.join(klist))\n",
    "     \n",
    "print(\"temp file saved in\",data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation du pourcentage de documents contenant le mot \"tramways\" dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = len(tramdoc)\n",
    "b = len(clean_list)\n",
    "x = (a / b)*100\n",
    "print(str(int(x)) + \"% of the documents in the corpus contains the word : 'tramways'\")\n",
    "\n",
    "labels = '\"tramways\" related docs ', 'Others'\n",
    "sizes = [x,(100-x)]\n",
    "colors = ['yellowgreen', 'gold']\n",
    "\n",
    "plt.pie(sizes, labels=labels, colors=colors, \n",
    "        autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date d'apparition dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "res = []\n",
    "\n",
    "for d in tramdoc:\n",
    "   datestring = ''.join(str(d))\n",
    "   match_str = re.search(r'\\d{4}', datestring)\n",
    "   res.append(datetime.strptime(match_str.group(), '%Y').date())\n",
    "\n",
    "res.sort()\n",
    "# printing result\n",
    "print(\"The word \" +\"'\"+\"tramways\"+\"' \"+\"first appear in the year : \" + str(res[0])[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF/IDF du mot \"tramways\" dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our corpus\n",
    "data = clean_list\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# convert text data into term-frequency matrix\n",
    "data = cv.fit_transform(data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# convert term-frequency matrix into tf-idf\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(data)\n",
    "\n",
    "# create dictionary to find a tfidf word each word\n",
    "word2tfidf = dict(zip(cv.get_feature_names(), tfidf_transformer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dict(word2tfidf)\n",
    "search_word = \"tramways\"\n",
    "for word, score in dictionary.items():\n",
    "    if search_word in dictionary:\n",
    "        print(\"TF/IDF score of: \"+ \"'\"+ search_word + \"'\" + \" is \" + str(score))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche des entités nommées présentes dans les documents qui parlent du tramways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le texte\n",
    "n=980000\n",
    "text = open(data_path + \"/\" + \"tramAll.txt\", encoding=\"ISO-8859-1\").read()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Traiter le texte\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "organisation = defaultdict(int)\n",
    "places = defaultdict(int)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 1\n",
    "    if ent.label_ == \"ORG\" and len(ent.text) > 3:\n",
    "        organisation[ent.text] += 1\n",
    "    if ent.label_ == \"LOC\" and len(ent.text) > 3:\n",
    "        places[ent.text] += 1\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier et imprimer\n",
    "\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_organisation = sorted(organisation.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_places = sorted(places.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "\n",
    "for person, freq in sorted_people:\n",
    "    print(f\"{person} (PER)apparait {freq} fois dans le corpus\")\n",
    "\n",
    "for organisation, freq in sorted_organisation:\n",
    "    print(f\"{organisation} (ORG)apparait {freq} fois dans le corpus\")\n",
    "\n",
    "for places, freq in sorted_places:\n",
    "    print(f\"{places} (LOC)apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération du WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path +\"/\"+\"tramAll.txt\" ), 'r') as f:\n",
    "    after = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = Counter(after.split())\n",
    "print(frequencies.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=1000, height=500, background_color='white').generate_from_frequencies(frequencies)\n",
    "cloud.to_file(os.path.join(data_path, \"tram.png\"))\n",
    "Image(filename=os.path.join(data_path, \"tram.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec sur le corpus réduit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sentences tokenization \n",
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding=\"ISO-8859-1\", errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = data_path+\"/tramAll.txt\"\n",
    "sentences = MySentences(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bigram_phrases.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigram_phrases.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ = list(bigram_phrases.vocab.keys())[144]\n",
    "print(key_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases.vocab[key_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(bigram_phraser[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phraser = Phraser(phrases_model=trigram_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(trigram_phraser[bigram_phraser[sentences]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Word2Vec(\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=32, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=10, # La taille du \"contexte\", ici 6 mots avant et après le mot observé\n",
    "    min_count=10, # On ignore les mots qui n'apparaissent pas au moins 10 fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en 4 threads\n",
    "    epochs=5 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descende de gradient, aka. epochs.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f\"../data/bulletinsTram.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"../data/bulletinsTram.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration du modèle Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"depenses\", \"tramways\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"travail\", \"tramways\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"transport\", \"tramways\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"engin\", \"tramways\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"rue\", \"tramways\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"rail\", \"tramways\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"voie\", \"tramways\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"route\", \"tramways\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"tramways\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"travail\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"tram\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"transport\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"rail\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de sentiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation des documents qui contiennent le mot \"tramways\" dans le corpus brut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_tramdoc = []\n",
    "def check_if_exists(x,y, ls):\n",
    "    if x and y in ls:\n",
    "        sents_tramdoc.append(ls)\n",
    "\n",
    "txtNum = os.listdir(txt_path)\n",
    "\n",
    "\n",
    "for g in txtNum:\n",
    "    with open(os.path.join(txt_path, g), 'r', encoding = \"ISO-8859-1\") as h:\n",
    "           hlist = [(h.read())]\n",
    "           word_tokenize(str(hlist))\n",
    "           hstring = ''.join(str(hlist))\n",
    "           check_if_exists('tramways','tramway', hstring)\n",
    " \n",
    "      \n",
    "print(\"Searching process done !\")\n",
    "with open(os.path.join(data_path, f'dirtyTram.txt'), 'w') as n:\n",
    "    n.write(' '.join(sents_tramdoc))\n",
    "    print(\"dirtyTram file saved in\",data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation des phrases contenant le mot \"tramways\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"../data/dirtyTram.txt\"\n",
    "outfile = \"../data/DirtyTramSents.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outfile, 'w', encoding=\"ISO-8859-1\") as output:\n",
    "    with open(infile, encoding=\"ISO-8859-1\", errors=\"backslashreplace\") as f:\n",
    "        content = f.readlines()\n",
    "        content = content[:LIMIT] if LIMIT is not None else content\n",
    "        n_lines = len(content)\n",
    "        for i, line in enumerate(content):\n",
    "            if i % 10000 == 0:\n",
    "                print(f'processing line {i}/{n_lines}')\n",
    "            sentences = sent_tokenize(line)\n",
    "            for sent in sentences:\n",
    "                output.write(sent + \"\\n\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_dirtyTram = []\n",
    "def check_if_exists(x,y, ls, s):\n",
    "    if x and y in ls:\n",
    "        sents_dirtyTram.append(s)\n",
    "\n",
    "for d in sentences:\n",
    "    word_tokenize(str(d))\n",
    "    vstring = ''.join(str(d))\n",
    "          \n",
    "    check_if_exists('tramways','tramway', vstring, d)\n",
    " \n",
    "      \n",
    "print(\"Searching process done !\")\n",
    "print(sents_dirtyTram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction d'analyse de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "def get_sentiment(input_text):\n",
    "    blob = tb(input_text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    polarity_perc = f\"{100*abs(polarity):.0f}\"\n",
    "    subjectivity_perc = f\"{100*subjectivity:.0f}\"\n",
    "    if polarity > 0:\n",
    "        polarity_str = f\"{polarity_perc}% positive\"\n",
    "    elif polarity < 0:\n",
    "        polarity_str = f\"{polarity_perc}% negative\"\n",
    "    else:\n",
    "        polarity_str = \"neutral\"\n",
    "    if subjectivity > 0:\n",
    "        subjectivity_str = f\"{subjectivity}% subjective\"\n",
    "    else:\n",
    "        subjectivity_str = \"perfectly objective\"\n",
    "    print(f\"This text is {polarity_str} and {subjectivity_str}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(str(sents_dirtyTram))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
