{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4: Travail Final \n",
    "## VANBELLE Julien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yake\n",
    "import collections\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import webtext\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('webtext')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from IPython.display import Image\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from tabulate import tabulate\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from datetime import datetime\n",
    "import dateutil.parser as dparser\n",
    "from dateutil.parser._parser import _timelex\n",
    "import re\n",
    "\n",
    "\n",
    "txt_path = '/Users/julienvanbelle/Documents/GitHub/tac/data/txt'\n",
    "data_path = '/Users/julienvanbelle/Documents/GitHub/tac/data'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(txt_path)\n",
    "data_bxl = [f for f in files if f.startswith('Bxl_')]\n",
    "len(data_bxl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for txt in data_bxl:\n",
    "    with open(os.path.join(txt_path, txt), 'r', encoding = \"ISO-8859-1\") as f:\n",
    "        data_list.append(f.read())\n",
    "\n",
    "len(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire tout le contenu dans un fichier temporaire pour le nettoyer des stopwords propore au corpus\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "with open(os.path.join(data_path, f'_temp.txt'), 'w') as f:\n",
    "    f.write(' '.join(data_list))\n",
    "    print(\"temp file saved in\",data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_words = webtext.words('{}/_temp.txt'.format(data_path))\n",
    "data_analysis = nltk.FreqDist(wt_words)\n",
    " \n",
    "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n",
    "data_analysis = nltk.FreqDist(filter_words)\n",
    "data_analysis.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addsw = []\n",
    "for z in sorted(filter_words):\n",
    "  if filter_words[z] > 25000:\n",
    "   addsw.append(z)\n",
    "\n",
    "print(addsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "swLower = ' '.join(str(e).lower() for e in addsw)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += addsw\n",
    "sw += swLower\n",
    "sw += \"conseil communal\", \"conseil général\", \"conseil supérieur\", \"administration communale\", \"conseil provincial\", \"l'administration communale\", \"conseil\", \"echevin\" , \"messieurs\", \"bruxelles\", \"bourgmestre\", \"collège\", \"être\"\n",
    "sw = set(sw)\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(u, folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\".txt\"\n",
    "        output_path = f\"_clean.txt\"\n",
    "    else:\n",
    "        ##for u in data_bxl:\n",
    "            input_path = f\"{folder}\" + \"/\" + u\n",
    "            output_path = data_path + \"/txt_cleaned/\" + \"Clean_\" + u ##opti ici avec {folder}\n",
    "            print(\"Cleaning ==> \" +u)\n",
    "    \n",
    "            output = open(output_path, \"w\", encoding=\"ISO-8859-1\")\n",
    "            with open(input_path, encoding=\"ISO-8859-1\") as g:\n",
    "                text = g.read()\n",
    "                words = nltk.wordpunct_tokenize(text)\n",
    "                kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "                kept_string = \" \".join(kept)\n",
    "                output.write(kept_string)\n",
    "            return f'Output has been written in {data_path + \"/txt_cleaned\"}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(data_path +\"/txt_cleaned\"):\n",
    "    os.mkdir(data_path + \"/txt_cleaned\")\n",
    "for x in data_bxl:\n",
    "    clean_text(x, folder=txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le résultat\n",
    "with open(os.path.join(data_path + \"/txt_cleaned/\" + \"Clean_\" + data_bxl[0]), 'r', encoding=\"ISO-8859-1\") as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path = data_path + \"/txt_cleaned/\"\n",
    "\n",
    "clean_files = os.listdir(clean_path)\n",
    "clean_list = []\n",
    "for j in clean_files:\n",
    "    with open(os.path.join(clean_path, j), 'r', encoding = \"ISO-8859-1\") as f:\n",
    "        clean_list.append(f.read())\n",
    "\n",
    "len(clean_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Répartition du corpus en clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tfidf_vectors = vectorizer.fit_transform(clean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 3\n",
    "\n",
    "km_model = KMeans(n_clusters=N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km_model.fit_predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(clusters):\n",
    "    clustering[label].append(clean_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##pprint(dict(clustering)) ##bug ici \n",
    "print(dict(clustering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de la thématique des tramways bruxellois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apparition du terme \"tramways\" dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tramdoc = []\n",
    "def check_if_exists(x, ls, r):\n",
    "    if x in ls:\n",
    "        ##print('\" ' + str(x) + ' \"' + ' is present in the document ' + r)\n",
    "        tramdoc.append(r)\n",
    "\n",
    "cleanNum = os.listdir(clean_path)\n",
    "\n",
    "for q in cleanNum:\n",
    "    with open(os.path.join(clean_path, q), 'r', encoding = \"ISO-8859-1\") as j:\n",
    "         jlist = [(j.read())]\n",
    "         word_tokenize(str(jlist))\n",
    "         jstring = ''.join(str(jlist))\n",
    "         \n",
    "         check_if_exists('tramways', jstring, q)\n",
    " \n",
    "      \n",
    "print(\"Searching process done !\")\n",
    "print(\"The word is present in \"+ str(len(tramdoc)) + \"/\" + str(len(clean_list)) + \" documents\")\n",
    "print(\"The word appears in \"+ str(tramdoc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation du pourcentage de documents contenant le mot \"tramways\" dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = len(tramdoc)\n",
    "b = len(clean_list)\n",
    "x = (a / b)*100\n",
    "print(str(int(x)) + \"% of the documents in the corpus contains the word : 'tramways'\")\n",
    "\n",
    "labels = '\"tramways\" related docs ', 'Others'\n",
    "sizes = [x,(100-x)]\n",
    "colors = ['yellowgreen', 'gold']\n",
    "\n",
    "plt.pie(sizes, labels=labels, colors=colors, \n",
    "        autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date d'apparition dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Clean_Bxl_%y_Tome_XX_Part_X.txt\"\n",
    "tramdate = []\n",
    "for d in tramdoc:\n",
    "   datestring = ''.join(str(d))\n",
    "   parsedate = dparser.parse(datestring,fuzzy=True)\n",
    "   tramdate.append(parsedate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##tramdate.sort(key=lambda date: datetime.strptime(date, \"%y\"))\n",
    "print(tramdate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF/IDF du mot \"tramways\" dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our corpus\n",
    "data = clean_list\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# convert text data into term-frequency matrix\n",
    "data = cv.fit_transform(data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# convert term-frequency matrix into tf-idf\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(data)\n",
    "\n",
    "# create dictionary to find a tfidf word each word\n",
    "word2tfidf = dict(zip(cv.get_feature_names(), tfidf_transformer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF/IDF score of: 'tramways' is 8.200051688984663\n"
     ]
    }
   ],
   "source": [
    "dictionary = dict(word2tfidf)\n",
    "search_word = \"tramways\"\n",
    "for word, score in dictionary.items():\n",
    "    if search_word in dictionary:\n",
    "        print(\"TF/IDF score of: \"+ \"'\"+ search_word + \"'\" + \" is \" + str(score))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Représentation graphique de la fréquence du mot \"tramways\" dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'tramways', 'Other words'\n",
    "sizes = [50,50]\n",
    "colors = ['yellowgreen', 'gold']\n",
    "\n",
    "plt.pie(sizes, labels=labels, colors=colors, \n",
    "        autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "\n",
    "plt.axis('equal')\n",
    "\n",
    "##plt.savefig('PieChart01.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de sentiment sur les phrases contenant le mot \"tramways\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "def get_sentiment(input_text):\n",
    "    blob = tb(input_text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    polarity_perc = f\"{100*abs(polarity):.0f}\"\n",
    "    subjectivity_perc = f\"{100*subjectivity:.0f}\"\n",
    "    if polarity > 0:\n",
    "        polarity_str = f\"{polarity_perc}% positive\"\n",
    "    elif polarity < 0:\n",
    "        polarity_str = f\"{polarity_perc}% negative\"\n",
    "    else:\n",
    "        polarity_str = \"neutral\"\n",
    "    if subjectivity > 0:\n",
    "        subjectivity_str = f\"{subjectivity}% subjective\"\n",
    "    else:\n",
    "        subjectivity_str = \"perfectly objective\"\n",
    "    print(f\"This text is {polarity_str} and {subjectivity_str}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Ce conseil municipal est vraiment super intéressant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recherche des entités nommées présentes dans les documents qui parlent du tramways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le texte\n",
    "n=900000\n",
    "text = open(\"../data/all.txt\", encoding='utf-8').read()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Traiter le texte\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier et imprimer\n",
    "\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for person, freq in sorted_people[:50]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = Counter(after.split())\n",
    "print(frequencies.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=2000, height=1000, background_color='white').generate_from_frequencies(frequencies)\n",
    "cloud.to_file(os.path.join(temp_path, f\"{year}.png\"))\n",
    "Image(filename=os.path.join(temp_path, f\"{year}.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
