{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4: Travail Final \n",
    "## VANBELLE Julien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/julienvanbelle/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /Users/julienvanbelle/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yake\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import webtext\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('webtext')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import Image\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from tabulate import tabulate\n",
    "\n",
    "txt_path = '/Users/julienvanbelle/Documents/GitHub/tac/data/txt'\n",
    "data_path = '/Users/julienvanbelle/Documents/GitHub/tac/data'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(txt_path)\n",
    "data_bxl = [f for f in files]\n",
    "len(data_bxl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for txt in data_bxl:\n",
    "    with open(os.path.join(txt_path, txt), 'r', encoding = \"ISO-8859-1\") as f:\n",
    "        data_list.append(f.read())\n",
    "\n",
    "len(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire tout le contenu dans un fichier temporaire\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "with open(os.path.join(data_path, f'_temp.txt'), 'w') as f:\n",
    "    f.write(' '.join(data_list))\n",
    "    print(\"temp file saved in\",data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_words = webtext.words('{}/_temp.txt'.format(data_path))\n",
    "data_analysis = nltk.FreqDist(wt_words)\n",
    " \n",
    "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n",
    "\n",
    "##for key in sorted(filter_words):\n",
    "    ##print(\"%s: %s\" % (key, filter_words[key]))\n",
    " \n",
    "data_analysis = nltk.FreqDist(filter_words)\n",
    "data_analysis.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addsw = []\n",
    "for z in sorted(filter_words):\n",
    "  if filter_words[z] > 25000:\n",
    "   ##print(\"%s: %s\" % (z, filter_words[z]))\n",
    "   addsw.append(z)\n",
    "\n",
    "print(addsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "swLower = ' '.join(str(e).lower() for e in addsw)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += addsw\n",
    "sw += swLower\n",
    "sw += \"conseil communal\", \"conseil général\", \"conseil supérieur\", \"administration communale\", \"conseil provincial\", \"l'administration communale\", \"conseil\", \"echevin\" , \"messieurs\", \"bruxelles\", \"bourgmestre\", \"collège\", \"être\"\n",
    "sw = set(sw)\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\".txt\"\n",
    "        output_path = f\"_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/_temp.txt\"\n",
    "        output_path = f\"{folder}/_clean.txt\"\n",
    "    output = open(output_path, \"w\", encoding=\"ISO-8859-1\")\n",
    "    with open(input_path, encoding=\"ISO-8859-1\") as f:\n",
    "        text = f.read()\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "        kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(folder=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le résultat\n",
    "with open(os.path.join(data_path, f'_clean.txt'), 'r', encoding=\"ISO-8859-1\") as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de la thématique des travaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=1.0,\n",
    "    min_df=0.5,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cld_data OK\n"
     ]
    }
   ],
   "source": [
    "cld_data = [open(f'{data_path}/_clean.txt',encoding=\"ISO-8859-1\").read()]\n",
    "\n",
    "print(\"cld_data OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 5s, sys: 6.64 s, total: 3min 11s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%time tfidf_vectors = vectorizer.fit_transform(cld_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x379056 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 379056 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prã           0.367100\n",
       "van           0.311157\n",
       "rue           0.289534\n",
       "collã         0.206637\n",
       "âªtre         0.155175\n",
       "                ...   \n",
       "jeuitfc       0.000001\n",
       "jeuke         0.000001\n",
       "jeulx         0.000001\n",
       "jeumaibien    0.000001\n",
       "kunziger      0.000001\n",
       "Length: 379056, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
