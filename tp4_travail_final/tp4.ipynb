{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4: Travail Final \n",
    "## VANBELLE Julien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yake\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import webtext\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('webtext')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import Image\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from tabulate import tabulate\n",
    "\n",
    "txt_path = '/Users/julienvanbelle/Documents/GitHub/tac/data/txt'\n",
    "data_path = '/Users/julienvanbelle/Documents/GitHub/tac/data'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(txt_path)\n",
    "data_bxl = [f for f in files if f.startswith('Bxl_')]\n",
    "len(data_bxl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for txt in data_bxl:\n",
    "    with open(os.path.join(txt_path, txt), 'r', encoding = \"ISO-8859-1\") as f:\n",
    "        data_list.append(f.read())\n",
    "\n",
    "len(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire tout le contenu dans un fichier temporaire pour le nettoyer des stopwords propore au corpus\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "with open(os.path.join(data_path, f'_temp.txt'), 'w') as f:\n",
    "    f.write(' '.join(data_list))\n",
    "    print(\"temp file saved in\",data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_words = webtext.words('{}/_temp.txt'.format(data_path))\n",
    "data_analysis = nltk.FreqDist(wt_words)\n",
    " \n",
    "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n",
    "data_analysis = nltk.FreqDist(filter_words)\n",
    "data_analysis.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addsw = []\n",
    "for z in sorted(filter_words):\n",
    "  if filter_words[z] > 25000:\n",
    "   addsw.append(z)\n",
    "\n",
    "print(addsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "swLower = ' '.join(str(e).lower() for e in addsw)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += addsw\n",
    "sw += swLower\n",
    "sw += \"conseil communal\", \"conseil général\", \"conseil supérieur\", \"administration communale\", \"conseil provincial\", \"l'administration communale\", \"conseil\", \"echevin\" , \"messieurs\", \"bruxelles\", \"bourgmestre\", \"collège\", \"être\"\n",
    "sw = set(sw)\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(u, folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\".txt\"\n",
    "        output_path = f\"_clean.txt\"\n",
    "    else:\n",
    "        ##for u in data_bxl:\n",
    "            input_path = f\"{folder}\" + \"/\" + u\n",
    "            output_path = data_path + \"/txt_cleaned/\" + \"Clean_\" + u ##opti ici avec {folder}\n",
    "            print(\"Cleaning ==> \" +u)\n",
    "    \n",
    "            output = open(output_path, \"w\", encoding=\"ISO-8859-1\")\n",
    "            with open(input_path, encoding=\"ISO-8859-1\") as g:\n",
    "                text = g.read()\n",
    "                words = nltk.wordpunct_tokenize(text)\n",
    "                kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "                kept_string = \" \".join(kept)\n",
    "                output.write(kept_string)\n",
    "            return f'Output has been written in {data_path + \"/txt_cleaned\"}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(data_path +\"/txt_cleaned\"):\n",
    "    os.mkdir(data_path + \"/txt_cleaned\")\n",
    "for x in data_bxl:\n",
    "    clean_text(x, folder=txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le résultat\n",
    "with open(os.path.join(data_path + \"/txt_cleaned/\" + \"Clean_\" + data_bxl[0]), 'r', encoding=\"ISO-8859-1\") as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path = data_path + \"/txt_cleaned/\"\n",
    "\n",
    "clean_files = os.listdir(clean_path)\n",
    "data_cleaned = [f for f in clean_files if f.startswith('Clean_Bxl_')]\n",
    "print(str(len(data_cleaned)) + \" files are in /txt_cleaned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de la thématique des travaux liè au tram à Bruxelles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des mots clés sur l'ensemble du corpus nettoyé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=100)\n",
    "kw_extractor\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "print(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyextract():\n",
    "    for f in sorted(data_cleaned):\n",
    "        text = open(os.path.join(clean_path, f), 'r', encoding=\"ISO-8859-1\").read()\n",
    "        keywords = kw_extractor.extract_keywords(text.lower())\n",
    "        kept = []\n",
    "        for kw, score in keywords:\n",
    "            words = kw.split()\n",
    "            if len(words) == 2 and kw.lower() not in sw:\n",
    "                kept.append(kw)\n",
    "        print(f\"{f} mentions these keywords: {', '.join(kept)}...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.map(keyextract())\n",
    "pool.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=1.0,\n",
    "    min_df=0.5,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld_data = [open(f'{data_path}/_clean.txt',encoding=\"ISO-8859-1\").read()]\n",
    "\n",
    "print(\"cld_data OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tfidf_vectors = vectorizer.fit_transform(cld_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
